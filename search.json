[
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About the Sandbox",
    "section": "",
    "text": "An infrastructure project for health data science training and research in Denmark\nThe National Health Data Science Sandbox project kicked off in 2021 with 5 years of funding via the Data Science Research Infrastructure initiative from the Novo Nordisk Foundation. Health data science experts at five Danish universities are contributing to the Sandbox with coordination from the Center for Health Data Science under lead PI Professor Anders Krogh. Data scientists hosted in the research groups of each PI are building infrastructure and training modules on Computerome and UCloud, the primary academic high performance computing (HPC) platforms in Denmark. If you have any questions or would like to get in touch with one of our data scientists, please contact us here.\n\n\n\n\n\nOur computational ‚Äòsandbox‚Äô allows data scientists to explore datasets, tools and analysis pipelines in the same high performance computing environments where real research projects are conducted. Rather than a single, hefty environment, we‚Äôre deploying modularized topical environments tailored for independent use on each HPC platform. We aim to support three key user groups based at Danish universities:\n\ntrainees: use our training modules to learn analysis techniques with some guidance and guardrails - for your data type of interest AND for general good practices for HPC environments\n\nresearchers: prototype your tools and algorithms with an array of good quality datasets that are GDPR compliant and free to access\neducators: develop your next course with computational assignments in the HPC environment your students will use for their research\n\nActivity developing independent training modules and hosting workshops has centered on UCloud, while collaborative construction of a flexible Course Platform has been completed on Computerome for use by the Sandbox and independent educators. Publicly sourced datasets are being used in training modules on UCloud, while generation of synthetic data is an ongoing project at Computerome. Sandbox resources are under active construction, so check out our other pages for the current status on HPC Access, Datasets, and Modules. We run workshops using completed training modules on a regular basis and provide active support for Sandbox-hosted courses through a slack workspace. See our Contact page for more information.\n\n\nPartner with the Sandbox\nThe Sandbox welcomes proposals for new courses, modules, and prototyping projects from researchers and educators. We‚Äôd like to partner with lecturers engaged with us in developing needed materials collaboratively - we would love to have input from subject experts or help promote exciting new tools and analysis methods via modules! Please contact us with your ideas at nhds_sandbox@sund.ku.dk.\n\nWe thank the Novo Nordisk Foundation for funding support. If you use the Sandbox for research or reference it in text or presentations, please acknowledge the Health Data Science Sandbox project and its funder the Novo Nordisk Foundation (grant number NNF20OC0063268)."
  },
  {
    "objectID": "modules/clinProteomics_0122.html",
    "href": "modules/clinProteomics_0122.html",
    "title": "Clinical Proteomics",
    "section": "",
    "text": ":fontawesome-brands-github: GitHub Repository\nUpdated: January 2021\nStatus: Under expansion\nThe general strategy for the clinical proteomics module is to provide software, computing resources, datsets and storage using UCloud. Written material (instructions etc.), example notebooks and other auxiliary files will be provided in a Github repository.\n\nProteomics Sandbox app will be used for GUI programs\n\nPrimarily for identification / quantification\nFragPipe / MSFragger for database search (and perhaps open search)\nPDV for visualizing spectral matches\nSearchGUI and PeptideShaker also available\n\nJupyterLab app for data analysis after quantification\n\nInit script to activate conda environment and install custom kernel\nNotebooks provided through Github (https://github.com/hds-sandbox/proteomics-course)\n\nDatasets, (installed) software and JSON config files stored in UCloud project folders\n\nStudents currently need to be project members\n\n\nIntended use: Self-guided introduction to basic proteomics\n!!! abstract ‚ÄúSyllabus‚Äù 1. Identify and quantify peptides/proteins * ‚ÄúDatabase search‚Äù using MSFragger/FragPipe or MaxQuant * Visualize peptide spectrum matches using e.g.¬†PDV, IDPicker, IPSA, ‚Ä¶ 2. Quality control analysis 3. Bioinformatics * Reintegrate clinical metadata * JupyterLab / RStudio + e.g.¬†PolySTest / VSClust / ‚Ä¶ 4. PhosphoProteomics\n!!! info ‚ÄúWorkshop requirements‚Äù Knowledge of Python and Jupyter Notebooks\n\n\n\nBMB online computational proteomics course\nNordBioNet summer school 2021 (workshops)\nIntroduction to bioinformatics for proteomics - Prof.¬†Harald Barsnes, University of Bergen\nQC workshop and Quantitative Analysis workshop, long 2019 version - Prof.¬†Veit Schwammle, University of Southern Denmark\nSimulation of proteomics data - Dr.¬†Marie Locard-Paulet, University of Copenhagen\nProteogenomics - Dr.¬†Marc Vaudel, University of Bergen\n\n\n\n\nCenter for Health Data Science, University of Copenhagen."
  },
  {
    "objectID": "modules/clinProteomics_0122.html#other-learning-resources",
    "href": "modules/clinProteomics_0122.html#other-learning-resources",
    "title": "Clinical Proteomics",
    "section": "",
    "text": "BMB online computational proteomics course\nNordBioNet summer school 2021 (workshops)\nIntroduction to bioinformatics for proteomics - Prof.¬†Harald Barsnes, University of Bergen\nQC workshop and Quantitative Analysis workshop, long 2019 version - Prof.¬†Veit Schwammle, University of Southern Denmark\nSimulation of proteomics data - Dr.¬†Marie Locard-Paulet, University of Copenhagen\nProteogenomics - Dr.¬†Marc Vaudel, University of Bergen\n\n\n\n\nCenter for Health Data Science, University of Copenhagen."
  },
  {
    "objectID": "modules/genomics.html",
    "href": "modules/genomics.html",
    "title": "Genomics",
    "section": "",
    "text": "Genomics\nGenomics is the study of genomes, the complete set of an organism‚Äôs DNA. Genomics research now encompasses functional and structural studies, epigenomics, and metagenomics, and genomic medicine is under active implementation and extension in the health sector.\nModules linked to genomics topics are currently under construction."
  },
  {
    "objectID": "modules/course_template.html",
    "href": "modules/course_template.html",
    "title": "\nCourse Template\n",
    "section": "",
    "text": "Course Template\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.\nJennifer Bartell Jose Alejandro Romero Herrera Samuele Soraggi\n!!! abstract ‚ÄúOverview‚Äù üí¨ Questions:\n- Why are we doing this?\n- What is the meaning of life?"
  },
  {
    "objectID": "modules/course_template.html#introduction",
    "href": "modules/course_template.html#introduction",
    "title": "\nCourse Template\n",
    "section": "Introduction",
    "text": "Introduction\n\nLook at this!\n\nLorem markdownum voluntas et praeteritae aliquando Cauno thyrso inevitabile est, interdum fingit educat, aliquo ungues solito sermo. Miscent pulveris me fletus moenia sed simul aequoris removit, te incursu.\n\nIncursurus liquidis Troiae\nEn ad staminaque ullam\nPellis erat collo Hectoreis cadavera quia draconem"
  },
  {
    "objectID": "modules/course_template.html#agenda",
    "href": "modules/course_template.html#agenda",
    "title": "\nCourse Template\n",
    "section": "Agenda",
    "text": "Agenda\n!!! note ‚ÄúIn this tutorial, we will cover:‚Äù * First item * Second item"
  },
  {
    "objectID": "modules/course_template.html#section-1",
    "href": "modules/course_template.html#section-1",
    "title": "\nCourse Template\n",
    "section": "Section 1",
    "text": "Section 1\nLorem markdownum voluntas et praeteritae aliquando Cauno thyrso inevitabile est, interdum fingit educat, aliquo ungues solito sermo. Miscent pulveris me fletus moenia sed simul aequoris removit, te incursu.\n??? question ‚ÄúHow does this thing work?‚Äù\nBy looking at it long enough!"
  },
  {
    "objectID": "modules/course_template.html#section-2",
    "href": "modules/course_template.html#section-2",
    "title": "\nCourse Template\n",
    "section": "Section 2",
    "text": "Section 2\nLorem markdownum voluntas et praeteritae aliquando Cauno thyrso inevitabile est, interdum fingit educat, aliquo ungues solito sermo. Miscent pulveris me fletus moenia sed simul aequoris removit, te incursu.\n??? note\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\nnulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa."
  },
  {
    "objectID": "modules/course_template.html#section-3",
    "href": "modules/course_template.html#section-3",
    "title": "\nCourse Template\n",
    "section": "Section 3",
    "text": "Section 3\nLorem markdownum voluntas et praeteritae aliquando Cauno thyrso inevitabile est, interdum fingit educat, aliquo ungues solito sermo. Miscent pulveris me fletus moenia sed simul aequoris removit, te incursu.\n!!! info ‚ÄúAre you using an Apple chip?‚Äù\nContinue to do so."
  },
  {
    "objectID": "modules/AlphaFold_0122.html",
    "href": "modules/AlphaFold_0122.html",
    "title": "AlphaFold",
    "section": "",
    "text": "AlphaFold\n:fontawesome-brands-github: GitHub Repository\nUpdated: January 2022\nStatus: Under expansion\nThis module will contain a basic standalone tutorial on how to run the newly implemented AlphaFold app in the Sandbox (UCloud version).\nIntended use: The aim of this repository is to on-board users for AlphaFold on Computerome/UCloud.\n!!! abstract ‚ÄúSyllabus‚Äù 1. Introduction to protein structural analysis 2. Evaluating predicted structures (AlphaFold DB) 3. Using the AlphaFold app to predict new structures (AlphaFold) 4. Replicating an AlphaFold study 5. Future developments possible with AlphaFold\n!!! info ‚ÄúWorkshop requirements‚Äù Knowledge of Python and Jupyter notebooks\n\nAcknowledgements\n\nCenter for Health Data Science, University of Copenhagen."
  },
  {
    "objectID": "access/genomedk.html",
    "href": "access/genomedk.html",
    "title": "Introduction to NGS data analysis",
    "section": "",
    "text": "sss"
  },
  {
    "objectID": "access/index.html",
    "href": "access/index.html",
    "title": "HPC access",
    "section": "",
    "text": "The Sandbox is collaborating with the two major academic high performance computing platforms in Denmark. Computerome is located at the Technical University of Denmark (and co-owned by the University of Copenhagen) while UCloud is owned by the University of Southern Denmark. These HPC platforms each have their own strengths which we leverage in the Sandbox in different ways."
  },
  {
    "objectID": "access/index.html#ucloud",
    "href": "access/index.html#ucloud",
    "title": "HPC access",
    "section": "UCloud",
    "text": "UCloud\nUCloud is a relatively new HPC platform that can be accessed by students at Danish universities (via a WAYF university login). It has a user friendly graphical user interface that supports straightforward project, user, and resource management. UCloud provides access to many tools via selectable Apps matched with a range of flexible compute resources, and the Sandbox is deploying training modules in this form such that any UCloud user can easily access Sandbox materials independently. The Sandbox is also hosting workshops and training events on UCloud in conjunction with in-person training.\n\n\n\n\n\n\nAccess Sandbox Apps on UCloud\n\n\n\nFind detailed instructions on accessing Sandbox apps here via UCloud. Check out UCloud‚Äôs extensive user docs here."
  },
  {
    "objectID": "access/index.html#computerome",
    "href": "access/index.html#computerome",
    "title": "HPC access",
    "section": "Computerome",
    "text": "Computerome\nComputerome is the home of many sensitive health datasets via collaborations between DTU, KU, Rigshospitalet, and other major health sector players in the Capital Region of Denmark. Computerome has recently launched their secure cloud platform, DELPHI, and in collaboration with the Sandbox has built a Course Platform on the same backbone such that courses and training can be conducted in the same environment as real research would be performed in the secure cloud. The Sandbox is supporting courses in the Course Platform, but it is also available for independent use by educators at Danish universities. Please see their website for more information on independent use and pricing, and contact us if you‚Äôd like to collaborate on hosting a course on Computerome. We can help with tool installation, environment testing, and user support (ranging from using the environment to course content if we have Sandbox staff with matching expertise).\nParticipants in courses co-hosted by the Sandbox can check here for access instructions."
  },
  {
    "objectID": "access/index.html#genomedk",
    "href": "access/index.html#genomedk",
    "title": "HPC access",
    "section": "GenomeDK",
    "text": "GenomeDK\nIn development."
  },
  {
    "objectID": "access/index.html#any-other-computing-cluster",
    "href": "access/index.html#any-other-computing-cluster",
    "title": "HPC access",
    "section": "Any other computing cluster",
    "text": "Any other computing cluster\nIn development."
  },
  {
    "objectID": "access/index.html#your-local-pc",
    "href": "access/index.html#your-local-pc",
    "title": "HPC access",
    "section": "Your local PC",
    "text": "Your local PC\nIn development."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Next Generation Sequencing data",
    "section": "",
    "text": "Computing and didactical support from the Danish Health Data Science Sandbox\n\nThis course introduces you to NGS data (short-reads and long-reads) alignment, variant analysis, bulk-RNA analysis and single-cell RNA analysis.\n\n\nThe material for this course is organized in four separated jupyter notebooks in both bash, python and R where you will benefit of an interactive coding setup on jupyterlab.\n\n\nThe first exercise lesson is executed with the web interface usegalaxy.org. Click on 1.Galaxy Exercise in the menu Exercises to get started)\n\n\n\nThe following exercise lessons will work on a computing environment with jupyterlab. Use the menu Access and the drop-down menu selecting the computing environment you need (danish clusters uCloud and GenomeDK, your PC, or another cluster).\n\n\n\n\nif you need to have a look at the exercises as a reference, then the menu Exercises contains all the compiled exercises on jupyterlab in a document format, from which you can also copy-paste the code.\n¬†\n\n¬†\n\n\n\n\n\n\nCourse overview\n\n\n\n\nAbstract: After the course, you will be able to apply bioinformatics methods for analyzing genomes and transcriptomes using NGS data. This includes knowledge of the existing types of genome data, how they can be displayed and analyzed, the current methods for genome assembly and analysis, their accuracy and how they can be used.\nPrerequisites: This is an introductory course that needs a basic understanding of the biology behind sequencing, and just basic programming experience would help.\nSyllabus:\n\nDescribe key challenges in the analysis of NGS data\nExplain the theoretical foundation for methods that use NGS for assembly and analysis of genomes\nDiscuss the bioinformatic methods for genome analysis and hypothesize what drives the outcome of the methods\nReview original literature within the subjects and relate the discussed topics to analysis scenarios\nApply bioinformatics tools within the selected application areas and reflect on the results, formulating your own conclusion in the proposed tasks\n\nTime: 20 hours (for reading through the code, executing it, answering questions). The material fits 4-5 days of lessons.\nSupporting Materials:\n\njupyter notebooks for interactive coding\nlecture slides from the instructor (Slides button in the menu)\n\nCourse authors\n\nMikkel H Schierup\nStig U Andersen\nSamuele Soraggi\nPeter S Porsborg\nAdri√°n G Repoll√©s\n\nLicense: Course Content is licensed under Creative Commons Attribution 4.0 International License\nCitation: If you use any of this material for your research, please cite this course with the DOI below, and acknowledge the Health Data Science Sandbox project of the Novo Nordisk Foundation (grant number NNF20OC0063268). It is of great help to support the project. \nContact: Samuele Soraggi (samuele at birc.au.dk) for technical issues in using the material."
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Introduction to Next Generation Sequencing data",
    "section": "",
    "text": "The material for this course is organized in four separated jupyter notebooks in both bash, python and R where you will benefit of an interactive coding setup on jupyterlab.\n\n\nThe first exercise lesson is executed with the web interface usegalaxy.org. Click on 1.Galaxy Exercise in the menu Exercises to get started)\n\n\n\nThe following exercise lessons will work on a computing environment with jupyterlab. Use the menu Access and the drop-down menu selecting the computing environment you need (danish clusters uCloud and GenomeDK, your PC, or another cluster)."
  },
  {
    "objectID": "index.html#compiled-exercises",
    "href": "index.html#compiled-exercises",
    "title": "Introduction to Next Generation Sequencing data",
    "section": "",
    "text": "if you need to have a look at the exercises as a reference, then the menu Exercises contains all the compiled exercises on jupyterlab in a document format, from which you can also copy-paste the code.\n¬†\n\n¬†\n\n\n\n\n\n\nCourse overview\n\n\n\n\nAbstract: After the course, you will be able to apply bioinformatics methods for analyzing genomes and transcriptomes using NGS data. This includes knowledge of the existing types of genome data, how they can be displayed and analyzed, the current methods for genome assembly and analysis, their accuracy and how they can be used.\nPrerequisites: This is an introductory course that needs a basic understanding of the biology behind sequencing, and just basic programming experience would help.\nSyllabus:\n\nDescribe key challenges in the analysis of NGS data\nExplain the theoretical foundation for methods that use NGS for assembly and analysis of genomes\nDiscuss the bioinformatic methods for genome analysis and hypothesize what drives the outcome of the methods\nReview original literature within the subjects and relate the discussed topics to analysis scenarios\nApply bioinformatics tools within the selected application areas and reflect on the results, formulating your own conclusion in the proposed tasks\n\nTime: 20 hours (for reading through the code, executing it, answering questions). The material fits 4-5 days of lessons.\nSupporting Materials:\n\njupyter notebooks for interactive coding\nlecture slides from the instructor (Slides button in the menu)\n\nCourse authors\n\nMikkel H Schierup\nStig U Andersen\nSamuele Soraggi\nPeter S Porsborg\nAdri√°n G Repoll√©s\n\nLicense: Course Content is licensed under Creative Commons Attribution 4.0 International License\nCitation: If you use any of this material for your research, please cite this course with the DOI below, and acknowledge the Health Data Science Sandbox project of the Novo Nordisk Foundation (grant number NNF20OC0063268). It is of great help to support the project. \nContact: Samuele Soraggi (samuele at birc.au.dk) for technical issues in using the material."
  },
  {
    "objectID": "datasets/datapolicy.html",
    "href": "datasets/datapolicy.html",
    "title": "Data policy",
    "section": "",
    "text": "A priority of the Sandbox is to guide health data science learning using real-world-similar datasets. A major component is addressing how to analyze and leverage person-specific data, such as electronic health records, without invading personal privacy or straying from GDPR guidelines on sensitive data use. We are therefore focused on using either publicly accessible datasets (that are generally well anonymized to enable such release) or we are using/creating synthetic datasets that mimic real-world datasets without replicating real people‚Äôs data such that they can be identified. In either case, it is essential for Sandbox users to treat person-specific data respectfully and be aware of the additional responsibility and limitations of working with this type of data as part of their career in health data science.\nWe recommend that users interested in this type of data complete an ethics course on research using health datasets before digging into any analysis. A well regarded course that is also often required for using public databases that contain person-specific data is the Human Subject and Data Research Ethics course designed by the Massachusetts Institute of Technology. The course is hosted at CITI, the Collaborative Institutional Training Initiative. Completing the course is free of charge and provides you with a certificate which you may need to upload to certain databases to gain access. Set up an account at CITI, add an Institutional affiliation with ‚ÄòMassachusetts Institute of Technology Affiliates‚Äô, and then find and complete the course titled ‚ÄòData or Specimens Only Research‚Äô to obtain a certificate (in pdf form)."
  },
  {
    "objectID": "datasets/datapolicy.html#with-respect-to-person-specific-datasets",
    "href": "datasets/datapolicy.html#with-respect-to-person-specific-datasets",
    "title": "Data policy",
    "section": "",
    "text": "A priority of the Sandbox is to guide health data science learning using real-world-similar datasets. A major component is addressing how to analyze and leverage person-specific data, such as electronic health records, without invading personal privacy or straying from GDPR guidelines on sensitive data use. We are therefore focused on using either publicly accessible datasets (that are generally well anonymized to enable such release) or we are using/creating synthetic datasets that mimic real-world datasets without replicating real people‚Äôs data such that they can be identified. In either case, it is essential for Sandbox users to treat person-specific data respectfully and be aware of the additional responsibility and limitations of working with this type of data as part of their career in health data science.\nWe recommend that users interested in this type of data complete an ethics course on research using health datasets before digging into any analysis. A well regarded course that is also often required for using public databases that contain person-specific data is the Human Subject and Data Research Ethics course designed by the Massachusetts Institute of Technology. The course is hosted at CITI, the Collaborative Institutional Training Initiative. Completing the course is free of charge and provides you with a certificate which you may need to upload to certain databases to gain access. Set up an account at CITI, add an Institutional affiliation with ‚ÄòMassachusetts Institute of Technology Affiliates‚Äô, and then find and complete the course titled ‚ÄòData or Specimens Only Research‚Äô to obtain a certificate (in pdf form)."
  },
  {
    "objectID": "datasets/datapolicy.html#public-domain-data",
    "href": "datasets/datapolicy.html#public-domain-data",
    "title": "Data policy",
    "section": "Public domain data",
    "text": "Public domain data\nThe intended scope of the Sandbox is broad, and we will be pulling from many different public access databases (especially for training modules on omics analysis). Databases can be topically broad, giant repositories or field-specific, and each may have its own usage rules. We plan to provide our own copies of publically available datasets where allowed to ensure compatibility with the linked module is preserved, but some datasets may need to be downloaded by users themselves under specific access / distribution restrictions. Many omics datasets do not present significant data sensitivity concerns in comparison to real-world data such as electronic health records (EHRs) and clinical trial datasets.\nThere are large public de-identified EHR datasets that serve as benchmark resources for teaching and comparing new methods with old, but these are not numerous and often have restricted usage and sharing terms in addition to being quite dated. Historical approaches to dataset anonymization and de-identification have been substantially challenged in the age of digitalized healthcare and increasing data integration, which means meaningfully large ‚Äòanonymized‚Äô datasets are now rarely released."
  },
  {
    "objectID": "datasets/datapolicy.html#synthetic-data",
    "href": "datasets/datapolicy.html#synthetic-data",
    "title": "Data policy",
    "section": "Synthetic data",
    "text": "Synthetic data\n\n\n\n\n\n\nVia our collaborators and broader network, the Sandbox has the opportunity to simulate/synthesize data resembling different databases and registries from the Danish health sector. We are exploring methods of creating useful synthetic datasets with national and EU-level data access policies and GDPR restrictions in mind, while developing initial datasets using publicly available data from Danish research studies and other resources.\nUltimately, a new era of synthetic data is rapidly developing. The funded Sandbox proposal focused on generating synthetic data using mechanistic models, agent-based models, or draws from multivariate distributions (such as copulas), which are methods that do not present any significant GDPR-related concerns with sharing the produced datasets as they are derived from population-level characteristics and prior knowledge. However, new deep learning-based methods of data synthesis can theoretically learn complex, nonlinear patterns within a sensitive dataset and generate a synthetic dataset that replicates these patterns. This is a really promising approach for sharing high utility synthetic datasets, but it also elevates risk of accidentally sharing too much about the real dataset and skirting the boundaries of GDPR and ethical data handling. There is an inherent trade-off between privacy preservation and similarity of the synthetic dataset to the original dataset, with method development focused on moving closer to the ideal zone of high privacy AND high similarity. The figure at right is a rough approximation of this relationship versus current families of synthesis methods.\nPlease see Synthetic Data for more information about our approach to this technology."
  },
  {
    "objectID": "datasets/datasets.html",
    "href": "datasets/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets\nHere we provide details of datasets used in our various modules as well as a specific guide on using electronic health record datasets."
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "A priority of the Sandbox is to guide health data science learning using real-world-similar datasets. A major component is addressing how to analyze and leverage person-specific data, such as electronic health records, without invading personal privacy or straying from GDPR guidelines on sensitive data use. We are therefore focused on using either publicly accessible datasets (that are generally well anonymized to enable such release) or we are using/creating synthetic datasets that mimic real-world datasets without replicating real people‚Äôs data such that they can be identified. In either case, it is essential for Sandbox users to treat person-specific data respectfully and be aware of the additional responsibility and limitations of working with this type of data as part of their career in health data science.\nWe recommend that users interested in this type of data complete an ethics course on research using health datasets before digging into any analysis. A well regarded course that is also often required for using public databases that contain person-specific data is the Human Subject and Data Research Ethics course designed by the Massachusetts Institute of Technology. The course is hosted at CITI, the Collaborative Institutional Training Initiative. Completing the course is free of charge and provides you with a certificate which you may need to upload to certain databases to gain access. Set up an account at CITI, add an Institutional affiliation with ‚ÄòMassachusetts Institute of Technology Affiliates‚Äô, and then find and complete the course titled ‚ÄòData or Specimens Only Research‚Äô to obtain a certificate (in pdf form).\n\n\nThe intended scope of the Sandbox is broad, and we will be pulling from many different public access databases in our development of teaching modules. There are classical datasets that serve as benchmark resources for teaching and comparing new methods with old, and also brand new datasets that will support modules on emerging technologies (such as spatial single cell RNA-seq analysis). Databases can be topically broad giant repositories or field-specific, and each may have its own usage rules. We plan to provide our own copies of publically available datasets where allowed to ensure compatibility with the linked module is preserved, but some datasets may need to be downloaded by users themselves under specific access / distribution restrictions.\n\n\n\nThe Sandbox is focused on supporting Danish health data science education and research. Via our collaborators and broader network, we have the opportunity to simulate/synthesize data resembling different databases and registries from the Danish health sector in addition to using traditional data simulation techniques to replicate general datasets. We are exploring methods of creating useful synthetic datasets with local access guidelines/GDPR restrictions in mind, while developing initial datasets using published data from Danish studies and publically available resources."
  },
  {
    "objectID": "datasets/index.html#public-domain-data",
    "href": "datasets/index.html#public-domain-data",
    "title": "Datasets",
    "section": "",
    "text": "The intended scope of the Sandbox is broad, and we will be pulling from many different public access databases in our development of teaching modules. There are classical datasets that serve as benchmark resources for teaching and comparing new methods with old, and also brand new datasets that will support modules on emerging technologies (such as spatial single cell RNA-seq analysis). Databases can be topically broad giant repositories or field-specific, and each may have its own usage rules. We plan to provide our own copies of publically available datasets where allowed to ensure compatibility with the linked module is preserved, but some datasets may need to be downloaded by users themselves under specific access / distribution restrictions."
  },
  {
    "objectID": "datasets/index.html#syntheticsimulated-data",
    "href": "datasets/index.html#syntheticsimulated-data",
    "title": "Datasets",
    "section": "",
    "text": "The Sandbox is focused on supporting Danish health data science education and research. Via our collaborators and broader network, we have the opportunity to simulate/synthesize data resembling different databases and registries from the Danish health sector in addition to using traditional data simulation techniques to replicate general datasets. We are exploring methods of creating useful synthetic datasets with local access guidelines/GDPR restrictions in mind, while developing initial datasets using published data from Danish studies and publically available resources."
  },
  {
    "objectID": "datasets/synthdata.html",
    "href": "datasets/synthdata.html",
    "title": "Synthetic data",
    "section": "",
    "text": "It is necessary to clarify what we mean when we refer to synthetic data within the Sandbox project. While the term has been used for decades to describe all kinds of ‚Äònon-real‚Äô data including those derived from models and simulations, developments in deep generative modeling have dramatically expanded our understanding of what synthetic data can be. In the age of deepfakes and news articles written entirely by ChatGPT, synthetic data derived from deep learning is in a wholly different class from data simulated with a mechanistic or agent-based model.\nThe Sandbox is actually interested in any form of synthetic data - our highest priority is providing safe-to-use data to trainees and researchers that does not raise any concerns about sensitive data with respect to the EU‚Äôs General Data Protection Regulation (GDPR) and local Danish data regulations. So, we are using both old school and new school forms of data synthesis. However, the discussion on this page is heavily weighted towards our interest in new school synthesis - with our connections to generative modeling researchers and high quality data, we are naturally interested in figuring out a safe way to deploy synthetic datasets derived from deep learning and other high similarity approaches.\n\n\n\n\n\n\nThe TLDR for synthetic data in the Sandbox\n\n\n\n- The development of synthetic datasets should be viewed as a research project. The technology is generally untested with few examples of public roll-out, and its deployment should be future-proofed as much as possible against attacks and potential sensitive data disclosure.\n- Synthetic data generation and evaluation approaches should be tailored     to each dataset of interest. With current technology, it is unlikely that high quality, safe-to-share datasets will be produced at any kind of production scale without a massive effort devoted to pre-processing, data harmonization, and customized routines for different families of datasets.\n- The Sandbox is not openly sharing any synthetic datasets generated from person-specific sensitive data. We think these datasets will be useful to approved researchers that ideally gain access via an approved data portal with registration and data use agreements with relevant data authorities. We are not currently that portal.\n\n\n\n\n\nWe have explored the performance of copulas, multiple imputation, sequential synthesis, and several generative adversarial network (GAN) approaches with a cancer dataset which we were developing for a course in the MS in Personal Medicine program at University of Copenhagen. We quickly discovered that factors such as missingness, collinearity, and the ratio of patients to features cause just as many problems for synthetic data generation as they do in predictive modeling. We are currently evaluating the above techniques as well as additional deep learning approaches such as variational autoencoders (VAEs) and Bayesian graphs against a collection of benchmark health datasets to better understand the positives and negatives of each technique when faced with common challenges in real world health data.\nRecently, a few interesting libraries / pipelines have been released that enable testing of different synthetic data generation approaches alongside a range of evaluation metrics. We are actively exploring these tools as we test different generation approaches and examining their implementation of evaluation metrics. We plan to add additional components and features as we resolve challenges with different target datasets.\n\n\n\nThere are 3 key principles to consider when judging the overall quality of a synthetic dataset: fidelity to the original dataset, risk to privacy, and prediction utility. Fidelity and utility are often grouped together as similarity to the original data which exists in a trade-off with privacy - the more similar your synthetic dataset to the original, the higher your risk to patient privacy. However, the distinction between them is important as they can be achieved independently of each other depending on the project frame. Fidelity refers to reproduction of the multivariate shape and structure of the original data (including complex nonlinear relationships) while utility refers to how well the synthetic dataset matches the predictive accuracy of the original dataset. Risk to privacy includes both risk of patient reidentification and risk of sensitive information disclosure about a patient. There are many proposed evaluation metrics for measuring different aspects of these three qualities. We are actively investigating the performance of these metrics against our different datasets.\n\nWe should point out that while using quantitative metrics to assess privacy preservation is a critical step in creating a synthetic dataset, positive results do not absolve us from any concerns regarding risk to privacy in the synthetic data. Regulatory guidelines regarding the safety of synthetic data and the ability to openly share it are extremely unclear. No authorities have specified quantitative cut-offs using these metrics that enable open release, for example. For this reason, we have developed our own internal guidelines for how to handle this aspect of the project, which are based on a comprehensive examination of relevant EU and Danish legislation (i.e.¬†the GDPR, the Artificial Intelligence Act, the Danish Health Law, and the Danish Data Protection Act). We continue work on synthesis with hope that new legislation such as the development of the European Health Data Space will provide further guidance in the future.\n\n\n\nWe are currently focused on exploring methods and metrics by developing reproducible, well documented examples and use cases of synthetic data in partnership with other researchers, legal advisors, and data authorities. We‚Äôre relying primarily on publicly available tabular health datasets in this exploration phase, but we will also work with sensitive data in the future. Our rules aim to preserve the trust of the public in how their health data is handled by data authorities and researchers.\n\n\n\n\n\n\nSandbox Rules for Synthetic Data\n\n\n\n1. Creation of synthetic data involves processing sensitive data, and this requires obtaining project approvals from data authorities when performing this work on sensitive data. Any synthetic data work with restricted-access, sensitive data by the Sandbox will only be conducted with these approvals in place in the frame of a research project.\n2. Goals for each synthetic dataset project should be defined at project initiation: how will the synthetic dataset be used, who is the intended audience, and how might it be shared? This frame should govern every consequent decision for that dataset and be shared alongside the final dataset.\n3. Quantitative metrics for fidelity, utility, and privacy preservation should be implemented for each dataset and shared alongside the final dataset.\n4. A cost-benefit analysis should be performed after the project is completed - is any risk to privacy appropriately balanced by value of the dataset in achieving its stated aims and contributing to the public good?\n5. Data authorities with ethical and strategic stakes in who accesses the synthetic dataset should be included in decisions about how it is used and who is allowed to access it. \n6. Synthetic datasets created from person-specific sensitive data rather than population characteristics can still pose privacy risks, and any users of the dataset should be approved and registered. The Sandbox will not release any such datasets publicly and will instead work with appropriate data authorities to decide how such datasets should be governed in a responsible way."
  },
  {
    "objectID": "datasets/synthdata.html#defining-synthetic-data",
    "href": "datasets/synthdata.html#defining-synthetic-data",
    "title": "Synthetic data",
    "section": "",
    "text": "It is necessary to clarify what we mean when we refer to synthetic data within the Sandbox project. While the term has been used for decades to describe all kinds of ‚Äònon-real‚Äô data including those derived from models and simulations, developments in deep generative modeling have dramatically expanded our understanding of what synthetic data can be. In the age of deepfakes and news articles written entirely by ChatGPT, synthetic data derived from deep learning is in a wholly different class from data simulated with a mechanistic or agent-based model.\nThe Sandbox is actually interested in any form of synthetic data - our highest priority is providing safe-to-use data to trainees and researchers that does not raise any concerns about sensitive data with respect to the EU‚Äôs General Data Protection Regulation (GDPR) and local Danish data regulations. So, we are using both old school and new school forms of data synthesis. However, the discussion on this page is heavily weighted towards our interest in new school synthesis - with our connections to generative modeling researchers and high quality data, we are naturally interested in figuring out a safe way to deploy synthetic datasets derived from deep learning and other high similarity approaches.\n\n\n\n\n\n\nThe TLDR for synthetic data in the Sandbox\n\n\n\n- The development of synthetic datasets should be viewed as a research project. The technology is generally untested with few examples of public roll-out, and its deployment should be future-proofed as much as possible against attacks and potential sensitive data disclosure.\n- Synthetic data generation and evaluation approaches should be tailored     to each dataset of interest. With current technology, it is unlikely that high quality, safe-to-share datasets will be produced at any kind of production scale without a massive effort devoted to pre-processing, data harmonization, and customized routines for different families of datasets.\n- The Sandbox is not openly sharing any synthetic datasets generated from person-specific sensitive data. We think these datasets will be useful to approved researchers that ideally gain access via an approved data portal with registration and data use agreements with relevant data authorities. We are not currently that portal."
  },
  {
    "objectID": "datasets/synthdata.html#generating-synthetic-data",
    "href": "datasets/synthdata.html#generating-synthetic-data",
    "title": "Synthetic data",
    "section": "",
    "text": "We have explored the performance of copulas, multiple imputation, sequential synthesis, and several generative adversarial network (GAN) approaches with a cancer dataset which we were developing for a course in the MS in Personal Medicine program at University of Copenhagen. We quickly discovered that factors such as missingness, collinearity, and the ratio of patients to features cause just as many problems for synthetic data generation as they do in predictive modeling. We are currently evaluating the above techniques as well as additional deep learning approaches such as variational autoencoders (VAEs) and Bayesian graphs against a collection of benchmark health datasets to better understand the positives and negatives of each technique when faced with common challenges in real world health data.\nRecently, a few interesting libraries / pipelines have been released that enable testing of different synthetic data generation approaches alongside a range of evaluation metrics. We are actively exploring these tools as we test different generation approaches and examining their implementation of evaluation metrics. We plan to add additional components and features as we resolve challenges with different target datasets."
  },
  {
    "objectID": "datasets/synthdata.html#evaluating-synthetic-data",
    "href": "datasets/synthdata.html#evaluating-synthetic-data",
    "title": "Synthetic data",
    "section": "",
    "text": "There are 3 key principles to consider when judging the overall quality of a synthetic dataset: fidelity to the original dataset, risk to privacy, and prediction utility. Fidelity and utility are often grouped together as similarity to the original data which exists in a trade-off with privacy - the more similar your synthetic dataset to the original, the higher your risk to patient privacy. However, the distinction between them is important as they can be achieved independently of each other depending on the project frame. Fidelity refers to reproduction of the multivariate shape and structure of the original data (including complex nonlinear relationships) while utility refers to how well the synthetic dataset matches the predictive accuracy of the original dataset. Risk to privacy includes both risk of patient reidentification and risk of sensitive information disclosure about a patient. There are many proposed evaluation metrics for measuring different aspects of these three qualities. We are actively investigating the performance of these metrics against our different datasets.\n\nWe should point out that while using quantitative metrics to assess privacy preservation is a critical step in creating a synthetic dataset, positive results do not absolve us from any concerns regarding risk to privacy in the synthetic data. Regulatory guidelines regarding the safety of synthetic data and the ability to openly share it are extremely unclear. No authorities have specified quantitative cut-offs using these metrics that enable open release, for example. For this reason, we have developed our own internal guidelines for how to handle this aspect of the project, which are based on a comprehensive examination of relevant EU and Danish legislation (i.e.¬†the GDPR, the Artificial Intelligence Act, the Danish Health Law, and the Danish Data Protection Act). We continue work on synthesis with hope that new legislation such as the development of the European Health Data Space will provide further guidance in the future."
  },
  {
    "objectID": "datasets/synthdata.html#rules-for-synthetic-data-in-the-sandbox",
    "href": "datasets/synthdata.html#rules-for-synthetic-data-in-the-sandbox",
    "title": "Synthetic data",
    "section": "",
    "text": "We are currently focused on exploring methods and metrics by developing reproducible, well documented examples and use cases of synthetic data in partnership with other researchers, legal advisors, and data authorities. We‚Äôre relying primarily on publicly available tabular health datasets in this exploration phase, but we will also work with sensitive data in the future. Our rules aim to preserve the trust of the public in how their health data is handled by data authorities and researchers.\n\n\n\n\n\n\nSandbox Rules for Synthetic Data\n\n\n\n1. Creation of synthetic data involves processing sensitive data, and this requires obtaining project approvals from data authorities when performing this work on sensitive data. Any synthetic data work with restricted-access, sensitive data by the Sandbox will only be conducted with these approvals in place in the frame of a research project.\n2. Goals for each synthetic dataset project should be defined at project initiation: how will the synthetic dataset be used, who is the intended audience, and how might it be shared? This frame should govern every consequent decision for that dataset and be shared alongside the final dataset.\n3. Quantitative metrics for fidelity, utility, and privacy preservation should be implemented for each dataset and shared alongside the final dataset.\n4. A cost-benefit analysis should be performed after the project is completed - is any risk to privacy appropriately balanced by value of the dataset in achieving its stated aims and contributing to the public good?\n5. Data authorities with ethical and strategic stakes in who accesses the synthetic dataset should be included in decisions about how it is used and who is allowed to access it. \n6. Synthetic datasets created from person-specific sensitive data rather than population characteristics can still pose privacy risks, and any users of the dataset should be approved and registered. The Sandbox will not release any such datasets publicly and will instead work with appropriate data authorities to decide how such datasets should be governed in a responsible way."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Course slides",
    "section": "",
    "text": "Here you find a table with the instructor‚Äôs slides from 2022.\n\n\n\nTopic\nSlide\nNotebook\n\n\n\n\nSequencing technologies\nlink\n‚Äì\n\n\nMapping to reference\nlink\nNotebook\n\n\nData visualization\nlink\n‚Äì\n\n\nSNPs and structural variants\nlink\nNotebook\n\n\nRNA sequencing\nlink\nNotebook\n\n\nDe-novo assembly\nlink\n‚Äì\n\n\nMicrobiomes and metagenomics\nlink\n‚Äì\n\n\nSingle cell RNA sequencing\nlink\nNotebook\n\n\n\n\n\n\nHere you find a table with the instructor‚Äôs slides and a link to the compiled notebooks, that you can also run on your own following the instructions in this webpage. Data alignment can also be performed on the Galaxy interactive webpage (see the galaxy exercise in this webpage)."
  },
  {
    "objectID": "slides.html#course-material-2022",
    "href": "slides.html#course-material-2022",
    "title": "Course slides",
    "section": "",
    "text": "Here you find a table with the instructor‚Äôs slides from 2022.\n\n\n\nTopic\nSlide\nNotebook\n\n\n\n\nSequencing technologies\nlink\n‚Äì\n\n\nMapping to reference\nlink\nNotebook\n\n\nData visualization\nlink\n‚Äì\n\n\nSNPs and structural variants\nlink\nNotebook\n\n\nRNA sequencing\nlink\nNotebook\n\n\nDe-novo assembly\nlink\n‚Äì\n\n\nMicrobiomes and metagenomics\nlink\n‚Äì\n\n\nSingle cell RNA sequencing\nlink\nNotebook"
  },
  {
    "objectID": "slides.html#course-material-2024",
    "href": "slides.html#course-material-2024",
    "title": "Course slides",
    "section": "",
    "text": "Here you find a table with the instructor‚Äôs slides and a link to the compiled notebooks, that you can also run on your own following the instructions in this webpage. Data alignment can also be performed on the Galaxy interactive webpage (see the galaxy exercise in this webpage)."
  },
  {
    "objectID": "access/UCloud.html",
    "href": "access/UCloud.html",
    "title": "UCloud",
    "section": "",
    "text": "Accessing the NGS summer school on UCloud\n1. User accounts on UCloud are enabled by university login credentials using WAYF (Where Are You From). Access the WAYF login portal with the button below here, and then find your affiliated university or institution using the search bar.\n¬†\n\n UCloud Access - click here \n\n¬†\n\n\n\n\n\n\nNGS summer school\n\n\n\nIf you are participating in the NGS Summer Course 2024 in Aarhus, click AFTER logging in on the button below. This will add you to a project on uCloud, where we have data and extra computing credit for the course. You should see a message on your browser where you have to accept the invitation to the project. The link expires 30 days after the course.\n¬†\n\n Invite link for the course \n\n¬†\n\n\n2. Once you are an approved user of UCloud, you are met with a dashboard interface as below. Here you can see a summary of the workspace you are using, like the hours of computing, the storage available, and other informations. The workspace you are working on is shown in the top-right corner (red circle). On the left side of the screen you have a toolbar menu.\n\n¬†\n\n\n\n\n\n\nNGS summer school\n\n\n\nIf you are participating in the NGS Summer Course 2024 in Aarhus, choose the workspace NGS summer school.\n\n\n3. The side toolbar menu menu can be used to access the stored data, applications, running programs and settings. Let‚Äôs have a look at the files section before we start running softwares.\n\n\n\n\n\n\nExercise: drives and file explorer\n\n\n\nClick on the first button of the left menu (Files). The Files window will open. Here you can create drives, which are virtual storage units resembling the drives of a computer, where you can store, delete and move files around.\nChoose the workspace My workspace and create a drive using the button on top of the window (Create drive), choose the resources available from the dropdown menu (in the workspace My workspace you probably have u1-cephfs) and assign a name.\nNow you have created your drive inside your personal workspace! You can create other drives, for example to keep projects separate. If you click into the drive, you can see new top buttons, for example to upload a file, create a folder and sync with another computer (which needs a software - click on the Sync button to read more). When you have a drive, you can mount it into a software you want to use (mount means to make it available for the software to read/write from and to).\nRemember to go back to the workspace you were working with before this exercise.\n\n\n4. The left-side can also be used to access applications, running programs and settings. Use the Applications symbol (red circle) and click on the Health Science store (green circle).\n\n5. Your screen will show some softwaress falling in the Health Science subcategory. Click on the Genomics Sandbox application to open its settings.\n\n6. Choose\n\nany Job Name (Nr 1 in the figure below, optional but useful) to identify the session,\nhow many hours you want to use for the job (Nr 2, choose at least 2 hours, you can increase this later)\nhow many CPUs (Nr 3, choose at least 4 CPUs for the first three exercises, but use at least 8 CPUs to run the single cell analysis).\nthe course Introduction to NGS Data Analysis (Nr 4).\n\nThen click on Submit (Nr 5).\n\n\n\n\n\n\n\nReuse parameters\n\n\n\nIf you want to run again a session in the same way as previously done, click on the button import parameters. This shows a list of previous sessions, from which all parameters choices can be imported.\n\n\n7. Once you have submitted, you end up in a queue, where you waiting resource assignment. The waiting time can vary, depending on how many people are using the computing cluster at the moment. When you get through the queue, you see a screen similar to the one below, where you have\n\nremaining time (red circle) which can be increased on-the-fly through the additional buttons (blue circle)\na stop button (purple circle) which you can hold to stop your session if you have finished ahead of remaining time. When you stop a session, you avoid using more resources than necessary, which will not be accounted for as used in your account credit!\na button to open the interactive interface (green button)\n\n\n8. Click on Open interface. You will start a new tab of your browser, in which the interface of Jupyterlab is opened.\n\n\n\n\n\n\nfind again your sessions\n\n\n\nDo not worry if you accidentally close the tab from which you can add time to your session. You can always find all your active and stopped sessions in the left toolbar of ucloud, under Jobs. Here you can click on your current session and see again the tab you closed!\n\n\n9. The Jupyterlab interface looks as below. It has a launcher from which you can start notebooks (documents including code and outputs) and terminals from scratch. On the left-side of the screen you have a file browser. You want to use the notebooks for this course, which you can find under the folder Notebooks.\n\n10. Now you can choose the notebooks. They are ordered by course topic and contain both explanations and code which you can run to see what happens. Double-click on the first notebook: you can see there are instructions to make sure the notebook works, and at some point you start see some code cells between the text, as in figure below.\n\n11. Cells always need to run from the first to the last in the order they have been written. To run a cell, select it and click the arrow-like symbol in the toolbar:\n\nAlternatively, press Shift+Enter on your keyboard, which is much more practical. Every time a code cell is running, a star simbol will show on its side. When it is done, a number will show, telling you the step number of the code you have been running. If a command shows an output, it will be visualized directly on the notebook. Use the toolbar to save the notebook, and edit it in any way you need adding text notes or code (you can add new cells via the toolbar).\n\n\nAccess again previously used notebooks and data\nIf you need to reuse the notebooks and the data used in a previous session, this can be done easily. Everything you produced during the notebook execution, and the notebooks themselves, are saved in your personal drive of the workspace. This has a name of the type NameSurname#Number. You need to add the folders Data and Notebooks from you previous session. Follow these steps:\n¬†\n1. Configure again the Genomics app as you did before, eventually using the button import parameters to choose the same configuration.\n2. Do not submit yet. Click twice on Add Folders - Two browsing bars will show up.\n\n3."
  },
  {
    "objectID": "modules/transcriptomics.html",
    "href": "modules/transcriptomics.html",
    "title": "Transcriptomics",
    "section": "",
    "text": "Transcriptomics\nTranscriptomics is the study of RNA transcripts and provides insight into gene expression patterns. State-of-the-art approaches rely on high-throughput sequencing of transcripts sampled by various methods."
  },
  {
    "objectID": "modules/EHRs.html",
    "href": "modules/EHRs.html",
    "title": "EHRs",
    "section": "",
    "text": "Electronic Health Records\nElectronic health records (EHRs) are digital records kept in the public health sector that record the medical histories of individuals, and access is normally highly restricted to preserve patient privacy. This data is sometimes also shared (partly or in full) in secondary patient registries that support research of a specific disease or condition (such as cystic fibrosis). These datasets are extraordinarily valuable in the development of predictive models used in precision medicine.\nModules linked to EHR analysis are currently under development."
  },
  {
    "objectID": "modules/index.html",
    "href": "modules/index.html",
    "title": "Training modules",
    "section": "",
    "text": "Sandbox resources have been organized as training modules focused on key topics in health data science. We are constantly adding additional resources and have plans to create additional modules on medical imaging and wearable device data. Feel free to adapt these resources for your own purposes (with credit to the National Health Data Science Sandbox project and other projects they acknowledge in the specific materials).\nYou can access our training modules through:"
  },
  {
    "objectID": "modules/index.html#sandbox-hpc",
    "href": "modules/index.html#sandbox-hpc",
    "title": "Training modules",
    "section": "Data Carpentry and management",
    "text": "Data Carpentry and management\n\n\n\n\n\n\n\nComputing skills are an important foundation for health data science (and using the above training modules), but formal training is often lacking as biomedical researchers navigate increasingly difficult computational tasks in their projects. These skills range from programming to the use of high-performance computers (HPC) to proper research data management.\n\nRDM for biodata (workshop on how to handle NGS data following simple guidelines to increase the FAIRability of your data)\nHPC launch (workshop in development)\nHPC pipes (workshop in development)\nHeaDS DataLab workshop materials (workshops for programming and good practices developed by the Center for Health Data Science at the University of Copenhagen, which are sometimes co-taught by Sandbox staff! Includes R, python, bash, and git!)"
  },
  {
    "objectID": "modules/index.html#genomics",
    "href": "modules/index.html#genomics",
    "title": "Training modules",
    "section": "Genomics",
    "text": "Genomics\n\n\n\n\n\n\n\nGenomics is the study of genomes, the complete set of an organism‚Äôs DNA. Genomics research now encompasses functional and structural studies, epigenomics, and metagenomics, and genomic medicine is under active implementation and extension in the health sector.\nUse the Genomics Sandbox App on UCloud to explore the resources below:\n\nIntroduction to Next Generation Sequencing data (last update: June 2023)\nIntroduction to Population Genomics (implementation of a course by Prof.¬†Kasper Munch of Aarhus University) (last update: March 2023)\nIntroduction to GWAS (last update: March 2023)"
  },
  {
    "objectID": "modules/index.html#transcriptomics",
    "href": "modules/index.html#transcriptomics",
    "title": "Training modules",
    "section": "Transcriptomics",
    "text": "Transcriptomics\n\n\n\n\n\n\n\nTranscriptomics is the study of transcriptomes, which investigates RNA transcripts within a cell or tissue to determine what genes are being expressed and in what proportion. These RNA transcripts include mRNAs, tRNA, rRNA, and other non-coding RNA present in a cell.\nUse the Transcriptomics Sandbox App on UCloud to explore these resources:\n\nBulk RNAseq (last update: June 2023)\nSingle-Cell RNAseq (last update: May 2023)\nCirrocumulus (a popular tool for visualizing different types of RNA-seq data and downstream analysis)\nRNAseq in RStudio (RStudio session with pre-installed RNAseq analysis packages for exploring with your own uploaded data)"
  },
  {
    "objectID": "modules/index.html#proteomics",
    "href": "modules/index.html#proteomics",
    "title": "Training modules",
    "section": "Proteomics",
    "text": "Proteomics\n\n\n\n\n\n\n\nProteomics is the study of proteins that are produced by an organism. Proteomics allows us to analyze protein composition and structure, which have great importance in determining their function.\nUse the Proteomics Sandbox App on UCloud to explore pre-installed tools for proteomics analysis and other resources:\n\nProteomics Sandbox Documentation (last update: May 2023)\nIntroduction to Clinical Proteomics (course under development)\n\nWe also offer a tutorial on UCloud‚Äôs ColabFold app, a tool that allows predictions with AlphaFold2 or RoseTTAFold.\n\nColabFold Intro (last update: October 2022)"
  },
  {
    "objectID": "modules/index.html#EHC",
    "href": "modules/index.html#EHC",
    "title": "Training modules",
    "section": "Electronic Health Records",
    "text": "Electronic Health Records\n\n\n\n\n\n\n\nElectronic health records (EHRs) are digital records kept in the public health sector that record the medical histories of individuals, and access is normally highly restricted to preserve patient privacy. This data is sometimes also shared (partly or in full) in secondary patient registries that support research on a specific disease or condition (such as breast cancer or cystic fibrosis). These datasets are extraordinarily valuable in the development of predictive models used in precision medicine.\nThe chronic lymphocytic leukemia synthetic dataset listed below is generated solely from public data. It is of low utility, so we don‚Äôt recommend its use beyond the course it was designed for (with much explanation for the students on its construction and caveats). Please see Synthetic Data for more information.\n\nChronic Lymphocytic Leukemia synthetic dataset created for use in ‚ÄúFra realworld data til personlig medicin‚Äù, a course from the University of Copenhagen‚Äôs MS in Personlig Medicin (last update: January 2023)\nIntro to EHR analysis (workshop under development)"
  },
  {
    "objectID": "modules/bulk_rnaseq.html",
    "href": "modules/bulk_rnaseq.html",
    "title": "Bulk RNAseq",
    "section": "",
    "text": ":material-web-plus: Course Page\n\nThis workshop material includes a tutorial on how to approach RNAseq data, starting from your count matrix. Thus, the workshop only briefly touches upon laboratory protocols, library preparation, and experimental design of RNA sequencing experiments, mainly for the purpose of outlining considerations in the downstream bioinformatic analysis. This workshop is based on the materials developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC), a collection of modified tutorials from the DESeq2 and R language vignettes.\nIntended use: The aim of this repository is to run a comprehensive but introductory workshop on bulk-RNAseq bioinformatic analyses. Each of the modules of this workshop is accompanied by a powerpoint slideshow explaining the steps and the theory behind a typical bioinformatics analysis (ideally with a teacher). Many of the slides are annotated with extra information and/or point to original sources for extra reading material.\n\n\nBy the end of this workshop, you should be able to analyse your own bulk RNAseq count matrix:\n\nNormalize your data.\nExplore your samples with PCAs and heatmaps.\nPerform Differential Expression Analysis.\nAnnotate your results.\n\n!!! agenda ‚ÄúSyllabus‚Äù 1. Course Introduction 2. Setup 3. Experimental planning 4. Data Explanation 5. Preprocessing 6. RNAseq counts 7. Exploratory analysis 8. Differential Expression Analysis 9. Functional Analysis 10. Summarized workflow\n!!! info ‚ÄúWorkshop prerequisites‚Äù - Knowledge of R, Rstudio and Rmarkdown. It is recommended that you have at least followed our workshop R basics - Basic knowledge of RNAseq technology - Basic knowledge of data science and statistics such as PCA, clustering and statistical testing\n\n\n\nCenter for Health Data Science, University of Copenhagen.\nHugo Tavares, Bioinformatics Training Facility, University of Cambridge.\nSilvia Raineri, Center for Stem Cell Medicine (reNew), University of Copenhagen.\nHarvard Chan Bioinformatics Core (HBC), check out their github repo"
  },
  {
    "objectID": "modules/bulk_rnaseq.html#goals",
    "href": "modules/bulk_rnaseq.html#goals",
    "title": "Bulk RNAseq",
    "section": "",
    "text": "By the end of this workshop, you should be able to analyse your own bulk RNAseq count matrix:\n\nNormalize your data.\nExplore your samples with PCAs and heatmaps.\nPerform Differential Expression Analysis.\nAnnotate your results.\n\n!!! agenda ‚ÄúSyllabus‚Äù 1. Course Introduction 2. Setup 3. Experimental planning 4. Data Explanation 5. Preprocessing 6. RNAseq counts 7. Exploratory analysis 8. Differential Expression Analysis 9. Functional Analysis 10. Summarized workflow\n!!! info ‚ÄúWorkshop prerequisites‚Äù - Knowledge of R, Rstudio and Rmarkdown. It is recommended that you have at least followed our workshop R basics - Basic knowledge of RNAseq technology - Basic knowledge of data science and statistics such as PCA, clustering and statistical testing\n\n\n\nCenter for Health Data Science, University of Copenhagen.\nHugo Tavares, Bioinformatics Training Facility, University of Cambridge.\nSilvia Raineri, Center for Stem Cell Medicine (reNew), University of Copenhagen.\nHarvard Chan Bioinformatics Core (HBC), check out their github repo"
  },
  {
    "objectID": "modules/proteomics.html",
    "href": "modules/proteomics.html",
    "title": "Proteomics",
    "section": "",
    "text": "Proteomics\nProteomics is the study of proteins summed across a complete sample (ranging from a single cell to a whole organism). High-throughput measurement is conducted using mass spectrometry techniques and protein arrays, and provides insight into protein expression profiles and interactions."
  }
]